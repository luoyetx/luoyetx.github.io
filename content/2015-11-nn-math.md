Title: Basic Mathematics in Neural Networks
Date: 2015-11-16
Slug: nn-math
Category: Machine Learning


Recently, I was reading the paper [Notes on Convolution Neural Networks][cnn-note]. The first part of the paper is talking about the traditional neural network, which is multi-layer fully connected network. It discusses the basic feature of multi-layer network and some formulas to present the feedforward pass and backpropagation pass. All the formulas are very simple but lack of the details about how things work. I am writing down this blog to record my derivation of these formulas.

A single layer in traditional neural network can be defined by the input $x$, the output $u$, the weight $W$ and the bias $b$, which we have $x \in R^n$, $W \in R^{m \cdot n}$, $b \in R^m$ and $u \in R^m$. And a layer can be defined like a function below.

$$
u = W \cdot x + b
$$

We first need some derivative of this function which will be every helpful later. we first rewrite this function to each element of $u$.

$$
u_k = W_k \cdot x + b_k
$$

$W_k$ is the k-th row of weight matrix $W$. We need three partial derivative $\frac{\partial u_k}{\partial b}$, $\frac{\partial u_k}{\partial W}$ and $\frac{\partial u_k}{\partial x}$. We also need another parital derivative of the function $u_k = W_k \cdot f(x) + b_k$, and the partial derivative is $\frac{\partial u_k}{\partial x}$.

The first term is $\frac{\partial u_k}{\partial b}$.

$$
\frac{\partial u_k}{\partial b_i} =
    \begin{cases}
    0 \quad if \quad i \neq k \\
    1 \quad if \quad i = k \\
    \end{cases}
$$

$$
\frac{\partial u_k}{\partial b} =
    \begin{bmatrix}
    \frac{\partial u_k}{\partial b_1} \\
    . \\
    \frac{\partial u_k}{\partial b_k} \\
    . \\
    \frac{\partial u_k}{\partial b_m} \\
    \end{bmatrix} =
    \begin{bmatrix}
    0 \\
    . \\
    1 \\
    . \\
    0 \\
    \end{bmatrix} \in R^m
$$

which we have $1$ in $k$-th row.

The second term is $\frac{\partial u_k}{\partial W}$.

$$
\frac{\partial u_k}{\partial W_i} =
    \begin{cases}
    0 \quad if \quad i \neq k \\
    x^T \quad if \quad i = k \\
    \end{cases}
$$

$$
\frac{\partial u_k}{\partial W} =
    \begin{bmatrix}
    \frac{\partial u_k}{\partial W_1} \\
    . \\
    \frac{\partial u_k}{\partial W_k} \\
    . \\
    \frac{\partial u_k}{\partial W_m} \\
    \end{bmatrix} =
    \begin{bmatrix}
    0 \\
    . \\
    x^T \\
    . \\
    0 \\
    \end{bmatrix}
        \in R^{m \cdot n}
$$

which we have $x^T$ in $k$-th row.

The third term is $\frac{\partial u_k}{\partial x}$.

$$
\frac{\partial u_k}{\partial x} =
    \begin{bmatrix}
    \frac{\partial u_k}{\partial x_1} \\
    . \\
    . \\
    \frac{\partial u_k}{\partial x_n} \\
    \end{bmatrix} =
    \begin{bmatrix}
    W_k1 \\
    . \\
    . \\
    W_kn \\
    \end{bmatrix} = W_k^T \in R^{n}
$$

The fourth term is the partial derivative $\frac{\partial u_k}{\partial x}$ of function $u_k = W_k \cdot f(x) + b$.

$$
\frac{\partial u_k}{\partial x_i} = W_ki \cdot f^\prime(x_i)
$$

$$
\frac{\partial u_k}{\partial x} = W_k^T \circ f^\prime(x)
$$

the notation $\circ$ here is an element wise multiplication.

With the four derivative above, we can now derivate the backpropagation algorithm of traditioanl neural network. We define $l$-th layer's input $x^{l-1}$, the output $u^l$, the weights $W^l$ and the bias $b^l$, we also define the activation function $f$. And the neural network is combined with $L$ layers, and its input will be $x^0$ and the output will be $t = f(u^L)$. We also define the loss $E = \frac{1}{2} \cdot \| t - y\|_2^2$. The relationship of all these notations are listed below.

$$
x^{l} = f(u^l),\quad u^l = W^l \cdot x^{l-1} + b^l
$$

For gradient descent, we need to calculate $\frac{\partial E}{\partial W^l}$ and $\frac{\partial E}{\partial b^l}$, and it is all backpropagation algorithm about. We first calculate $\frac{\partial E}{\partial b^l}$ and define a notation $\delta$ to help calculate these two partial derivative.

$$
\delta^l = \frac{\partial E}{\partial u^l}
$$

for $l$-th layer, we are not care the dimension of the input and output, the infomation are all in the $x^l$ and $u^l$.

to calculate $\frac{\partial E}{\partial b^l}$, we use $\frac{\partial u_k}{\partial b}$

$$
\frac{\partial E}{\partial b^l} = \sum_{k}\frac{\partial E}{\partial u_k^l} \cdot \frac{\partial u_k^l}{\partial b^l},
$$

$$
= \sum_{k}\delta_k^l \cdot
    \begin{bmatrix}
    0 \\
    . \\
    1 \\
    . \\
    0 \\
    \end{bmatrix}
$$

$$
= \sum_{k}
    \begin{bmatrix}
    0 \\
    . \\
    \delta_k^l \\
    . \\
    0 \\
    \end{bmatrix} = \delta^l
$$

to calculate $\frac{\partial E}{\partial W^l}$, we use $\frac{\partial u_k}{\partial W}$

$$
\frac{\partial E}{\partial W^l} = \sum_{k}\frac{\partial E}{\partial u^l_k} \cdot \frac{\partial u^l_k}{\partial W^l},
$$

$$
= \sum_{k}\delta^l_k \cdot
    \begin{bmatrix}
    0 \\
    . \\
    {(x^{l-1})}^T \\
    . \\
    0 \\
    \end{bmatrix}
$$

$$
= \sum_{k}
    \begin{bmatrix}
    0 \\
    . \\
    \delta^l_k \cdot {(x^{l-1})}^T \\
    . \\
    0 \\
    \end{bmatrix} = \delta^l \cdot {(x^{l-1})}^T
$$

Now, we can calculate the gradient for parameters $W^l$ and $b^l$, but they all depend on $\delta^l$ which is the core of backpropagation algorithm. And the algorithm is all about how to calculate $\delta^l$ from higher layer to lower layer, and the highest layer is the neural network's output layer which we put the loss $E$ in the algorithm.

We first calculate $\delta^l$, using $\frac{\partial u_k}{\partial x}$ of function $u = W \cdot x + b$ and $\frac{\partial u_k}{\partial x}$ in function $u = W \cdot f(x) + b$, and we also have $u^{l+1} = W^{l+1} \cdot f(u^l) + b^{l+1}$

$$
\delta^l = \frac{\partial E}{\partial u^l} = \sum_{k}\frac{\partial E}{\partial u^{l+1}_k} \cdot \frac{\partial u^{l+1}_k}{\partial u^l}
$$

$$
= \sum_{k}\delta^{l+1}_k \cdot {(W^{l+1}_k)}^T \circ f^\prime(u^l)
$$

$$
= {(W^{l+1})}^T \cdot \delta^{l+1} \circ f^\prime(u^l)
$$

for the output layer, we have $t = f(u^L)$ and $E = \frac{1}{2} \cdot \| t - y\|_2^2$.

$$
\delta^L = \frac{\partial E}{\partial u^L} = f^\prime(u^L) \circ (t - y)
$$

Finally, we get all these notations solved, and given an input $x^0$ and a target $y$ attached to it, which we have $x^0 \in R^n$ and $y \in R^m$. We first forwrad neural network and get all $u^l$, then calculate the top layer $\delta^L$, after all, we backword the error from top to bottom to calculate $\delta^l$, meanwhile update $W^l$ and $u^l$. This is really how the backpropagation algorithm works.

Let me put all notations together.

feedforward

$$
x^{l} = f(u^l),\quad u^l = W^l \cdot x^{l-1} + b^l
$$

backpropagation

$$
\delta^L = f^\prime(u^L) \circ (t - y)
$$

$$
\delta^l = {(W^{l+1})}^T \cdot \delta^{l+1} \circ f^\prime(u^l)
$$

gradient of parameters

$$
\frac{\partial E}{\partial W^l} = \delta^l \cdot {(x^{l-1})}^T
$$

$$
\frac{\partial E}{\partial b^l} = \delta^l
$$

That's all.

### References

- [Notes on Convolution Neural Networks][cnn-note]


[cnn-note]: http://cogprints.org/5869/1/cnn_tutorial.pdf
